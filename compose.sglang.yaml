services:
  llmhost:
    image: lmsysorg/sglang:latest
    ipc: host
    shm_size: 32g
    environment:
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    volumes:
      - /home/tiny/mnt/drive/.cache/huggingface:/root/.cache/huggingface
      - /home/tiny/mnt/drive/.cache/sglang:/tmp/torchinductor_root
    restart: unless-stopped
    # --model neuralmagic/Phi-3-medium-128k-instruct-quantized.w8a8
    # --model unsloth/Llama-3.2-3B-Instruct
    # --model Qwen/Qwen2.5-14B-Instruct-AWQ
    # --model Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8
    # --chat-template /root/chat_template.jinja
    # --chat-template /root/qwenchat_template.jinja
    # --disable-log-requests
    # --disable-log-stats
    # --kv-cache-dtype fp8
    # --model Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8
    # --model Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
    #                   [--tool-call-parser {granite-20b-fc,granite,hermes,internlm,jamba,llama3_json,mistral,pythonic} or name registered in --tool-parser-plugin]
    # --enable-auto-tool-choice
    # --tool-call-parser hermes
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
    # --model neuralmagic/Mistral-Small-24B-Instruct-2501-FP8-Dynamic
    # --chat-template "{%- set default_system_message = '' %}\n\n{{- bos_token }}\n\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = default_system_message %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\n\n{%- for message in loop_messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '[INST]' + message['content'] + '[/INST]' }}\n    {%- elif message['role'] == 'system' %}\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- message['content'] + eos_token }}\n    {%- endif %}\n{%- endfor %}"
    # --kv-cache-dtype fp8
    # --enable-prefix-caching
    # --model cortecs/phi-4-FP8-Dynamic
    # --model Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8
    # --enable-auto-tool-choice
    # --tool-call-parser hermes
    # --served-model-name model
    # --max-model-len 16384
    # --kv-cache-dtype fp8
    # --guided-decoding-backend outlines
    # -tp 2
    command: >
      python3 -m sglang.launch_server
      --model unsloth/gpt-oss-20b
      --tp 2
      --port 8000
      --host 0.0.0.0
      --tool-call-parser gpt-oss
      --max-running-requests 4
    ports:
      - "8000:8000"
    devices:
      - nvidia.com/gpu=all
