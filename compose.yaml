version: '3.9'
services:
  openaiapi:
    image: localhost/agentflow
    build: ./agentflow
    ports:
      - "8003:8003"
    restart: unless-stopped
    env_file:
      - ./.agentflow.default.env
      - ./.agentflow.env
    depends_on:
      - llmhost
      - embedding-server
      - scraper
    environment:
      - EMBEDDING_URI=http://embedding-server:8000/embeddings
      - SCRAPER_URI=http://scraper:8002/scrape
      - OPENAI_URI=http://llmhost:8000
      - MODEL_NAME=
    tty: true
    command: server http://llmhost:8000 http://embedding-server:8000/embeddings http://scraper:8002/scrape model -v --prompt-dir=Prompts 
    volumes:
      - ./request_logs:/App/LlmRequestLogs

  llmhost:
    # image: ghcr.io/ggerganov/llama.cpp:server-cuda
    image: localhost/llama.cpp
    volumes:
      - ${MODEL_DIR}:/models
    # restart: unless-stopped
    command: >
      -m /models/${MODEL_NAME}
      --metrics
      --port 8000
      --host 0.0.0.0
      -ngl 100
      -fa
      -c 8192
      -ts 1,0
      --main-gpu 1
    ports:
      - "8000:8000"
    devices:
      - nvidia.com/gpu=all
    env_file:
      - ./.agentflow.default.env
      - ./.agentflow.env

  embedding-server:
    image: localhost/embedding-server
    build: ./embedding-server
    ports:
      - "8001:8000"
    devices:
      - "nvidia.com/gpu=all"
    restart: unless-stopped

  scraper:
    image: localhost/scraper
    build: ./scraper
    ports:
      - "8002:8002"
    restart: unless-stopped