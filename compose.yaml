services:
  openaiapi:
    image: localhost/agentflow
    build: ./agentflow
    ports:
      - "8003:8003"
    restart: unless-stopped
    env_file:
      - ./.agentflow.default.env
      - ./.agentflow.env
    depends_on:
      - llmhost
      - embedding-server
      - scraper
    environment:
      - EMBEDDING_URI=http://embedding-server:8000/embeddings
      - SCRAPER_URI=http://scraper:8002/scrape
      - OPENAI_URI=http://llmhost:8000
      - MODEL_NAME=
    tty: true
    command: server http://llmhost:8000 http://embedding-server:8000/embeddings http://scraper:8002/scrape model -v --prompt-dir=Prompts 
    volumes:
      - ./request_logs:/App/LlmRequestLogs

  # llmhost:
  #   # image: ghcr.io/ggerganov/llama.cpp:server-cuda
  #   image: localhost/llama.cpp
  #   volumes:
  #     - ${MODEL_DIR}:/models
  #   # restart: unless-stopped
  #   command: >
  #     -m /models/${MODEL_NAME}
  #     --metrics
  #     --port 8000
  #     --host 0.0.0.0
  #     -ngl 100
  #     -fa
  #     -c 8192
  #     -ts 1,0
  #     --main-gpu 1
  #   ports:
  #     - "8000:8000"
  #   devices:
  #     - nvidia.com/gpu=all
  #   env_file:
  #     - ./.agentflow.default.env
  #     - ./.agentflow.env
  
  llmhost:
    image: vllm/vllm-openai:v0.7.1
    # ipc: host
    shm_size: 1g
    environment:
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      # - VLLM_USE_V1=1
    volumes:
      - /home/tiny/mnt/drive/.cache/huggingface:/root/.cache/huggingface
    restart: unless-stopped
    # --model neuralmagic/Phi-3-medium-128k-instruct-quantized.w8a8
    # --model unsloth/Llama-3.2-3B-Instruct
    # --model Qwen/Qwen2.5-14B-Instruct-AWQ
    # --model Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8
    # --chat-template /root/chat_template.jinja
    # --chat-template /root/qwenchat_template.jinja
    # --disable-log-requests
    # --disable-log-stats
    # --kv-cache-dtype fp8
    # --model Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8
    # --model Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
    #                   [--tool-call-parser {granite-20b-fc,granite,hermes,internlm,jamba,llama3_json,mistral,pythonic} or name registered in --tool-parser-plugin]
    # --enable-auto-tool-choice
    # --tool-call-parser hermes
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
    # --model neuralmagic/Mistral-Small-24B-Instruct-2501-FP8-Dynamic
    # --chat-template "{%- set default_system_message = '' %}\n\n{{- bos_token }}\n\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = default_system_message %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\n\n{%- for message in loop_messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '[INST]' + message['content'] + '[/INST]' }}\n    {%- elif message['role'] == 'system' %}\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- message['content'] + eos_token }}\n    {%- endif %}\n{%- endfor %}"
    command: >
      --model cortecs/phi-4-FP8-Dynamic
      --served-model-name model
      --max-model-len 8192
      --kv-cache-dtype fp8
      -tp 2
    ports:
      - "8000:8000"
    devices:
      - nvidia.com/gpu=all
      # - nvidia.com/gpu=1


  embedding-server:
    image: localhost/embedding-server
    build: ./embedding-server
    ports:
      - "8001:8000"
    devices:
      - "nvidia.com/gpu=all"
    restart: unless-stopped

  scraper:
    image: localhost/scraper
    build: ./scraper
    ports:
      - "8002:8002"
    restart: unless-stopped