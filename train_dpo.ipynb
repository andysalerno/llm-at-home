{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets trl peft sentencepiece wandb triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_path = os.environ.get('PATH')\n",
    "\n",
    "os.environ['PATH'] = f'/home/andy/repos/monorepo/bin:{current_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForCausalLM\n",
    "from unsloth import FastMistralModel, PatchDPOTrainer\n",
    "PatchDPOTrainer()\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import DPOTrainer\n",
    "import wandb\n",
    "\n",
    "# Defined in the secrets tab in Google Colab\n",
    "wb_token = '<replace>'\n",
    "hf_token = '<replace>'\n",
    "wandb.login(key=wb_token)\n",
    "\n",
    "model_name = \"openchat/openchat-3.5-0106\"\n",
    "new_model = \"/home/andy/mnt/drive/trained/openchat-nectar-0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as format_as_chat(), but specially handles entries with multiple turns\n",
    "def format_as_chat(example):\n",
    "    prompt = example['prompt']\n",
    "    messages = parse_conversation(prompt)\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # in the Nectar dataset, each example has an 'answers' list of at most 7 answers\n",
    "    # each 'answer' has 'answer', 'model', and 'rank'\n",
    "    # 'answer' is the text, 'model' is the model that generated that text, and 'rank' is how gpt4-turbo ranks the answer\n",
    "    # since DPO simply wants a \"chosen\" and \"rejected\", we will pick the top ranked answer as \"chosen\",\n",
    "    # and the 3rd ranking answer as \"rejected\". We skip the 2nd to improve the signal between \"good\" and \"bad\" (in theory).\n",
    "    chosen = find_entry_with_rank(example['answers'], 1)\n",
    "\n",
    "    if len(example['answers']) > 2:\n",
    "        rejected = find_entry_with_rank(example['answers'], 3)\n",
    "    elif len(example['answers'] > 1):\n",
    "        rejected = find_entry_with_rank(example['answers'], 2)\n",
    "    else:\n",
    "        assert(False, 'Expected at least a rank 2 or 3 to exist')\n",
    "\n",
    "    assert(chosen['rank'] == 1)\n",
    "    assert(rejected['rank'] > 1)\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": chosen['answer'] + eos_token_text,\n",
    "        # question: should the rejected answer exclude the eos token, since we *do* was to highly value the eos token?\n",
    "        \"rejected\": rejected['answer'] + eos_token_text,\n",
    "    }\n",
    "\n",
    "def parse_conversation(text):\n",
    "    # Regular expression pattern to match '\\n\\nHuman:' or '\\n\\nAssistant:' followed by any text\n",
    "    pattern = r\"\\n\\n(Human|Assistant): (.*?)(?=\\n\\n(Human|Assistant): |\\Z)\"\n",
    "    \n",
    "    # Find all matches using the regular expression\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    # Initialize an empty list to store the parsed data\n",
    "    parsed_data = []\n",
    "\n",
    "    # Iterate over the matches and create a dictionary for each\n",
    "    for role, content, _ in matches:\n",
    "        if role == 'Human':\n",
    "            parsed_data.append({\"role\": \"user\", \"content\": content})\n",
    "        elif role == 'Assistant':\n",
    "            parsed_data.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "    assert(parsed_data[-1][\"role\"] == \"assistant\")\n",
    "    assert(len(parsed_data[-1][\"content\"]) == 0)\n",
    "\n",
    "    parsed_data.pop() # remove final, empty assistant response - it will be added back in the chat jinja turn formatting\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "def find_entry_with_rank(entries, rank):\n",
    "    \"\"\"Given a list of elements and a target rank, return the first element with that rank.\"\"\"\n",
    "    return next((elem for elem in entries if elem['rank'] == rank), None)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"berkeley-nest/Nectar\")['train']\n",
    "\n",
    "# Save columns\n",
    "original_columns = dataset.column_names\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "eos_token_text = tokenizer.eos_token\n",
    "print(f'eos token: {eos_token_text}')\n",
    "\n",
    "# Format dataset\n",
    "# weird: some entries don't have an answer with rank 1?\n",
    "dataset = dataset.filter(lambda x: any([e for e in x['answers'] if e['rank'] == 1]))\n",
    "dataset = dataset.map(\n",
    "    format_as_chat,\n",
    "    remove_columns=original_columns\n",
    ")\n",
    "\n",
    "# Print sample\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Supports automatic RoPE Scaling, so choose any number.\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# LoRA configuration\n",
    "# Not used with Unsloth\n",
    "# peft_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    "# )\n",
    "\n",
    "# Before unsloth:\n",
    "# Model to fine-tune\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     load_in_4bit=True\n",
    "# )\n",
    "# model.config.use_cache = False\n",
    "\n",
    "# With Unsloth:\n",
    "model, _ = FastMistralModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "\n",
    "# Before Unsloth:\n",
    "# Reference model\n",
    "# ref_model, _ = FastMistralModel.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     load_in_4bit=True\n",
    "# )\n",
    "# With Unsloth:\n",
    "# ref_model, _ = FastMistralModel.from_pretrained(\n",
    "#     model_name = model_name,\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "# )\n",
    "\n",
    "model = FastMistralModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Dropout = 0 is currently optimized\n",
    "    bias = \"none\",    # Bias = \"none\" is currently optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=1,\n",
    "    output_dir=new_model,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    # warmup_steps=100,\n",
    "    warmup_ratio=0.1,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "# Create DPO trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    # None, # ref_model can be none??\n",
    "    # ref_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # peft_config=peft_config, # not used with Unsloth\n",
    "    beta=0.1,\n",
    "    max_prompt_length=1024,\n",
    "    max_length=1536,\n",
    ")\n",
    "\n",
    "# Fine-tune model with DPO\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save artifacts\n",
    "# dpo_trainer.model.save_pretrained(\"final_checkpoint\")\n",
    "# tokenizer.save_pretrained(\"final_checkpoint\")\n",
    "\n",
    "# # Flush memory\n",
    "# del dpo_trainer, model #, ref_model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Reload model in FP16 (instead of NF4)\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     return_dict=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # Merge base model with the adapter\n",
    "# model = PeftModel.from_pretrained(base_model, \"final_checkpoint\")\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "# # Save model and tokenizer\n",
    "# model.save_pretrained(new_model)\n",
    "# tokenizer.save_pretrained(new_model)\n",
    "\n",
    "name = 'openchat-nectar-0.2'\n",
    "model = AutoModelForCausalLM.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "# Push them to the HF Hub\n",
    "model.push_to_hub(name, use_temp_dir=False, token=hf_token)\n",
    "tokenizer.push_to_hub(name, use_temp_dir=False, token=hf_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
