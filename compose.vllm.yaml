version: '3.9'
name: vllm-stack
services:
  llmhost:
    image: vllm/vllm-openai:v0.11.0
    # image: localhost/vllm-nightly:latest
    # ipc: host
    shm_size: 1g
    environment:
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      # - VLLM_LOGGING_LEVEL=DEBUG
      # - VLLM_TRACE_FUNCTION=1
      # - VLLM_ATTENTION_BACKEND=FLASHINFER
    volumes:
      - /home/tiny/mnt/drive/.cache/huggingface:/root/.cache/huggingface
      - ./.vllm-cache/:/root/.cache/vllm/
      - /home/tiny/mnt/drive/gguf/:/root/gguf
    restart: unless-stopped
    # qwen3-30B-A3B-Instruct-2507-W8A8:
    # --model ramblingpolymath/Qwen3-30B-A3B-Instruct-2507-W8A8
    # --tokenizer unsloth/Qwen3-30B-A3B-Instruct-2507
    # --tool-call-parser hermes
    # --enable-auto-tool-choice
    # --max-num-seqs 4
    # --served-model-name model
    # --max-model-len 32648
    # -tp 2

    # --model QuantTrio/Qwen3-30B-A3B-Thinking-2507-GPTQ-Int8
    # --tokenizer unsloth/Qwen3-30B-A3B-Thinking-2507
    # --reasoning-parser qwen3
    # --tool-call-parser hermes
    # --enable-auto-tool-choice
    # --max-num-seqs 4
    # --served-model-name model
    # --max-model-len 32648
    # -tp 2

    # --model openai/gpt-oss-20b
    # --max-num-seqs 4
    # --served-model-name model
    # -tp 2
    # --enable-auto-tool-choice
    # --tool-call-parser openai
    # --max-num-batched-tokens 8192
    # --max-model-len 65536

    # qwen30
    # --model QuantTrio/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8
    # --tokenizer unsloth/Qwen3-30B-A3B-Instruct-2507
    # --tool-call-parser hermes
    # --enable-auto-tool-choice
    # --max-num-seqs 4
    # --max-num-batched-tokens 8192
    # --served-model-name model
    # --max-model-len 65536
    # -tp 2
    command: >
      --model nvidia/NVIDIA-Nemotron-Nano-12B-v2
      --trust-remote-code
      --enable-auto-tool-choice
      --mamba_ssm_cache_dtype float32
      --max-num-seqs 4
      --max-num-batched-tokens 8192
      --served-model-name model
      --max-model-len 65536
      -tp 2
    ports:
      - "8000:8000"
    devices:
      - nvidia.com/gpu=all
