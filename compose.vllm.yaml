version: '3.9'
services:
  llmhost:
    image: vllm/vllm-openai:v0.8.3
    # ipc: host
    shm_size: 1g
    environment:
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      # - VLLM_ATTENTION_BACKEND=FLASHINFER
      # - VLLM_USE_V1=0
    volumes:
      - /home/tiny/mnt/drive/.cache/huggingface:/root/.cache/huggingface
      - ./.vllm-cache/:/root/.cache/vllm/
      - ./vllm-plugins/:/root/vllm-plugins/
      - ./chat-templates/:/root/chat-templates/
      # - /home/tiny/mnt/drive/gguf/:/root/gguf
    restart: unless-stopped
    # --model neuralmagic/Phi-3-medium-128k-instruct-quantized.w8a8
    # --model unsloth/Llama-3.2-3B-Instruct
    # --model Qwen/Qwen2.5-14B-Instruct-AWQ
    # --model Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8
    # --chat-template /root/chat_template.jinja
    # --chat-template /root/qwenchat_template.jinja
    # --disable-log-requests
    # --disable-log-stats
    # --kv-cache-dtype fp8
    # --model Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8
    # --model Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
    #                   [--tool-call-parser {granite-20b-fc,granite,hermes,internlm,jamba,llama3_json,mistral,pythonic} or name registered in --tool-parser-plugin]
    # --tool-call-parser hermes
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
    # --model neuralmagic/Mistral-Small-24B-Instruct-2501-FP8-Dynamic
    # --chat-template "{%- set default_system_message = '' %}\n\n{{- bos_token }}\n\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = default_system_message %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\n\n{%- for message in loop_messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '[INST]' + message['content'] + '[/INST]' }}\n    {%- elif message['role'] == 'system' %}\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- message['content'] + eos_token }}\n    {%- endif %}\n{%- endfor %}"
    # --kv-cache-dtype fp8
    # --enable-prefix-caching
    # --model cortecs/phi-4-FP8-Dynamic
    # --hf-config-path unsloth/gemma-3-27b-it-bnb-4bit
    # --model vinimuchulski/gemma-3-27b-it-qat-q4_0-gguf
    # --model ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g
    command: >
      --model gaunernst/gemma-3-27b-it-qat-compressed-tensors
      --chat-template /root/chat-templates/gemma3.jinja
      --tool-parser-plugin /root/vllm-plugins/gemma3_parser.py
      --tool-call-parser gemma
      --enable-auto-tool-choice
      --served-model-name model
      --guided-decoding-backend outlines
      --max-model-len 16000
      --enable-prefix-caching
      -tp 2
    ports:
      - "8000:8000"
    devices:
      - nvidia.com/gpu=all
