version: '3.9'
services:
  llmhost:
    image: vllm/vllm-openai:v0.8.3
    # ipc: host
    shm_size: 1g
    environment:
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      # - VLLM_USE_V1=1
    volumes:
      - /home/tiny/mnt/drive/.cache/huggingface:/root/.cache/huggingface
    restart: unless-stopped
    # --model neuralmagic/Phi-3-medium-128k-instruct-quantized.w8a8
    # --model unsloth/Llama-3.2-3B-Instruct
    # --model Qwen/Qwen2.5-14B-Instruct-AWQ
    # --model Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8
    # --chat-template /root/chat_template.jinja
    # --chat-template /root/qwenchat_template.jinja
    # --disable-log-requests
    # --disable-log-stats
    # --kv-cache-dtype fp8
    # --model Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8
    # --model Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
    #                   [--tool-call-parser {granite-20b-fc,granite,hermes,internlm,jamba,llama3_json,mistral,pythonic} or name registered in --tool-parser-plugin]
    # --enable-auto-tool-choice
    # --tool-call-parser hermes
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
    # --model neuralmagic/Mistral-Small-24B-Instruct-2501-FP8-Dynamic
    # --chat-template "{%- set default_system_message = '' %}\n\n{{- bos_token }}\n\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = default_system_message %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\n\n{%- for message in loop_messages %}\n    {%- if message['role'] == 'user' %}\n        {{- '[INST]' + message['content'] + '[/INST]' }}\n    {%- elif message['role'] == 'system' %}\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- message['content'] + eos_token }}\n    {%- endif %}\n{%- endfor %}"
    command: >
      --model cortecs/phi-4-FP8-Dynamic
      --served-model-name model
      --max-model-len 8192
      --kv-cache-dtype fp8
      --enable-prefix-caching
      -tp 2
    ports:
      - "8000:8000"
    devices:
      - nvidia.com/gpu=all
      # - nvidia.com/gpu=1
