version: '3.9'
services:
  llmhost:
    image: vllm/vllm-openai:v0.9.0
    # image: localhost/vllmgguf:latest
    # ipc: host
    shm_size: 1g
    environment:
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      # - VLLM_ATTENTION_BACKEND=FLASHINFER
      # - VLLM_USE_V1=1
    volumes:
      - /home/tiny/mnt/drive/.cache/huggingface:/root/.cache/huggingface
      - ./.vllm-cache/:/root/.cache/vllm/
      - ./vllm-plugins/:/root/vllm-plugins/
      - ./chat-templates/:/root/chat-templates/
      - /home/tiny/mnt/drive/gguf/:/root/gguf
    restart: unless-stopped
    # --model Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8
    # --model Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
    # --model Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4
    # --model vinimuchulski/gemma-3-27b-it-qat-q4_0-gguf
    # --model ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g
    # --model gaunernst/gemma-3-27b-it-qat-compressed-tensors
    # gemma3 below:
    # disable flash infer:
    # disable v1 or might get cuda oom:
    # --model ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g
    # --chat-template /root/chat-templates/gemma3.jinja
    # --tool-parser-plugin /root/vllm-plugins/gemma3_parser.py
    # --tool-call-parser gemma
    # --enable-auto-tool-choice
    # --served-model-name model
    # --max-model-len 16384
    # --limit_mm_per_prompt 'image=0'
    # -tp 2
    # gemma3-qat not working:
    # --model /root/gguf/google-gemma-3-27b-it-qat-q4_0-gguf-small/gemma-3-27b-it-q4_0_s.gguf
    # --tokenizer unsloth/gemma-3-27b-it-unsloth-bnb-4bit
    # --chat-template /root/chat-templates/gemma3.jinja
    # --tool-parser-plugin /root/vllm-plugins/gemma3_parser.py
    # --tool-call-parser gemma
    # --enable-auto-tool-choice
    # --served-model-name model
    # --guided-decoding-backend outlines
    # --max-model-len 16000
    # --enable-prefix-caching
    # gemma3 with compressed tensors:
    # --model gaunernst/gemma-3-27b-it-qat-compressed-tensors
    # or: --model leon-se/gemma-3-27b-it-FP8-Dynamic (disable flash infer)
    # --chat-template /root/chat-templates/gemma3.jinja
    # --tool-parser-plugin /root/vllm-plugins/gemma3_parser.py
    # --tool-call-parser gemma
    # --enable-auto-tool-choice
    # --served-model-name model
    # --guided-decoding-backend outlines
    # --max-model-len 16000
    # --enable-prefix-caching
    # -tp 2
    # mistral 3.1 small below:
    # --model ISTA-DASLab/Mistral-Small-3.1-24B-Instruct-2503-GPTQ-4b-128g
    # --tool-call-parser llama3_json
    # --chat-template examples/tool_chat_template_mistral_parallel.jinja
    # --enable-auto-tool-choice
    # --served-model-name model
    # --guided-decoding-backend outlines
    # --max-model-len 16000
    # --enable-prefix-caching
    # mistral 3.1 small gguf-8bit below:
    # --model /root/gguf/Mistral-Small-3.1-24B-Instruct-2503-GGUF/Mistral-Small-3.1-24B-Instruct-2503-Q8_0.gguf
    # --tokenizer unsloth/Mistral-Small-3.1-24B-Instruct-2503-bnb-4bit
    # --load-format mistral
    # --tool-call-parser mistral
    # --served-model-name model
    # --chat-template examples/tool_chat_template_mistral_parallel.jinja
    # --enable-auto-tool-choice
    # --guided-decoding-backend outlines
    # --max-model-len 16000
    # --enable-prefix-caching
    # -tp 2
    # mistral 3.1 small:
    # --model RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8
    # --tool-call-parser mistral
    # --served-model-name model
    # --chat-template examples/tool_chat_template_mistral_parallel.jinja
    # --enable-auto-tool-choice
    # --max-model-len 16000
    # --override-generation-config
    # --enable-prefix-caching
    # -tp 2
    # Qwen2.5-32B-4bit
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
    # --tool-call-parser hermes
    # --enable-auto-tool-choice
    # --served-model-name model
    # --guided-decoding-backend outlines
    # --max-model-len 16000
    # --enable-prefix-caching
    # -tp 2
    # Qwen2.5-32B-8bit (make sure to use FlashInfer)
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8
    # --kv-cache-dtype fp8
    # --tool-call-parser hermes
    # --guided-decoding-backend outlines
    # --enable-auto-tool-choice
    # --served-model-name model
    # --max-model-len 16000
    # -tp 2
    # Qwne3
    # RedHatAI/Qwen3-14B-FP8_dynamic
    # --model btbtyler09/Qwen3-30B-A3B-gptq-8bit

    # --model Qwen/Qwen3-30B-A3B-FP8
    # --reasoning-parser qwen3
    # --max-num-seqs 3
    # --tool-call-parser hermes
    # --enable-auto-tool-choice
    # --served-model-name model
    # --max-model-len 32648
    # -tp 2
    command: >
      --model Qwen/Qwen3-30B-A3B-FP8
      --reasoning-parser qwen3
      --max-num-seqs 4
      --tool-call-parser hermes
      --enable-auto-tool-choice
      --served-model-name model
      --max-model-len 32648
      -tp 2
    ports:
      - "8000:8000"
    devices:
      - nvidia.com/gpu=all
