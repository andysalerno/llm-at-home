version: '3.9'
name: vllm-stack
services:
  llmhost:
    image: vllm/vllm-openai:v0.11.0
    # image: localhost/vllm-nightly:latest
    # ipc: host
    shm_size: 1g
    environment:
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      # - VLLM_LOGGING_LEVEL=DEBUG
      # - VLLM_TRACE_FUNCTION=1
      # - VLLM_ATTENTION_BACKEND=FLASHINFER
      # - VLLM_USE_V1=1
    volumes:
      - /home/tiny/mnt/drive/.cache/huggingface:/root/.cache/huggingface
      - ./.vllm-cache/:/root/.cache/vllm/
      - /home/tiny/mnt/drive/gguf/:/root/gguf
    restart: unless-stopped
    # --model Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8
    # --model Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
    # --model Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4
    # --model vinimuchulski/gemma-3-27b-it-qat-q4_0-gguf
    # --model ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g
    # --model gaunernst/gemma-3-27b-it-qat-compressed-tensors
    # gemma3 below:
    # disable flash infer:
    # disable v1 or might get cuda oom:
    # --model ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g
    # --chat-template /root/chat-templates/gemma3.jinja
    # --tool-parser-plugin /root/vllm-plugins/gemma3_parser.py
    # --tool-call-parser gemma
    # --enable-auto-tool-choice
    # --served-model-name model
    # --max-model-len 16384
    # --limit_mm_per_prompt 'image=0'
    # -tp 2
    # gemma3-qat not working:
    # --model /root/gguf/google-gemma-3-27b-it-qat-q4_0-gguf-small/gemma-3-27b-it-q4_0_s.gguf
    # --tokenizer unsloth/gemma-3-27b-it-unsloth-bnb-4bit
    # --chat-template /root/chat-templates/gemma3.jinja
    # --tool-parser-plugin /root/vllm-plugins/gemma3_parser.py
    # --tool-call-parser gemma
    # --enable-auto-tool-choice
    # --served-model-name model
    # --guided-decoding-backend outlines
    # --max-model-len 16000
    # --enable-prefix-caching
    # gemma3 with compressed tensors:
    # --model gaunernst/gemma-3-27b-it-qat-compressed-tensors
    # or: --model leon-se/gemma-3-27b-it-FP8-Dynamic (disable flash infer)
    # --chat-template /root/chat-templates/gemma3.jinja
    # --tool-parser-plugin /root/vllm-plugins/gemma3_parser.py
    # --tool-call-parser gemma
    # --enable-auto-tool-choice
    # --served-model-name model
    # --guided-decoding-backend outlines
    # --max-model-len 16000
    # --enable-prefix-caching
    # -tp 2
    # mistral 3.1 small below:
    # --model ISTA-DASLab/Mistral-Small-3.1-24B-Instruct-2503-GPTQ-4b-128g
    # --tool-call-parser llama3_json
    # --chat-template examples/tool_chat_template_mistral_parallel.jinja
    # --enable-auto-tool-choice
    # --served-model-name model
    # --guided-decoding-backend outlines
    # --max-model-len 16000
    # --enable-prefix-caching
    # mistral 3.1 small gguf-8bit below:
    # --model /root/gguf/Mistral-Small-3.1-24B-Instruct-2503-GGUF/Mistral-Small-3.1-24B-Instruct-2503-Q8_0.gguf
    # --tokenizer unsloth/Mistral-Small-3.1-24B-Instruct-2503-bnb-4bit
    # --load-format mistral
    # --tool-call-parser mistral
    # --served-model-name model
    # --chat-template examples/tool_chat_template_mistral_parallel.jinja
    # --enable-auto-tool-choice
    # --guided-decoding-backend outlines
    # --max-model-len 16000
    # --enable-prefix-caching
    # -tp 2
    # mistral 3.1 small:
    # --model RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8
    # --tool-call-parser mistral
    # --served-model-name model
    # --chat-template examples/tool_chat_template_mistral_parallel.jinja
    # --enable-auto-tool-choice
    # --max-model-len 16000
    # --override-generation-config
    # --enable-prefix-caching
    # -tp 2
    # Qwen2.5-32B-4bit
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
    # --tool-call-parser hermes
    # --enable-auto-tool-choice
    # --served-model-name model
    # --guided-decoding-backend outlines
    # --max-model-len 16000
    # --enable-prefix-caching
    # -tp 2
    # Qwen2.5-32B-8bit (make sure to use FlashInfer)
    # --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8
    # --kv-cache-dtype fp8
    # --tool-call-parser hermes
    # --guided-decoding-backend outlines
    # --enable-auto-tool-choice
    # --served-model-name model
    # --max-model-len 16000
    # -tp 2
    # Qwne3
    # RedHatAI/Qwen3-14B-FP8_dynamic
    # --model btbtyler09/Qwen3-30B-A3B-gptq-8bit

    # --model Qwen/Qwen3-30B-A3B-FP8
    # --reasoning-parser qwen3
    # --max-num-seqs 3
    # --tool-call-parser hermes
    # --enable-auto-tool-choice
    # --served-model-name model
    # --max-model-len 32648
    # -tp 2
    # --guided-decoding-backend xgrammar
    # [--guided-decoding-backend {auto,guidance,lm-format-enforcer,outlines,xgrammar}]
    #
    # RedHatAI/gemma-3-27b-it-quantized.w8a8
    # mobiuslabsgmbh/Qwen3-32B_gemlite-ao_a16w4_gs_128_pack_32bit
    # btbtyler09/Qwen3-30B-A3B-gptq-8bit <== error
    # Qwen/Qwen3-30B-A3B-FP8
    # nytopop/Qwen3-30B-A3B.w8a8
    # --no-enforce-eager
    # --model QuantTrio/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8

    # qwen3-30B-A3B-Instruct-2507-W8A8:
    # --model ramblingpolymath/Qwen3-30B-A3B-Instruct-2507-W8A8
    # --tokenizer unsloth/Qwen3-30B-A3B-Instruct-2507
    # --tool-call-parser hermes
    # --enable-auto-tool-choice
    # --max-num-seqs 4
    # --served-model-name model
    # --max-model-len 32648
    # -tp 2

    # --model QuantTrio/Qwen3-30B-A3B-Thinking-2507-GPTQ-Int8
    # --tokenizer unsloth/Qwen3-30B-A3B-Thinking-2507
    # --reasoning-parser qwen3
    # --tool-call-parser hermes
    # --enable-auto-tool-choice
    # --max-num-seqs 4
    # --served-model-name model
    # --max-model-len 32648
    # -tp 2

      # --model openai/gpt-oss-20b
      # --max-num-seqs 4
      # --served-model-name model
      # -tp 2
      # --enable-auto-tool-choice
      # --tool-call-parser openai
      # --max-num-batched-tokens 8192
      # --max-model-len 65536

      # qwen30
      # --model QuantTrio/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8
      # --tokenizer unsloth/Qwen3-30B-A3B-Instruct-2507
      # --tool-call-parser hermes
      # --enable-auto-tool-choice
      # --max-num-seqs 4
      # --max-num-batched-tokens 8192
      # --served-model-name model
      # --max-model-len 65536
      # -tp 2
    command: >
      --model unsloth/granite-4.0-h-small-unsloth-bnb-4bit
      --tool-call-parser granite
      --enable-auto-tool-choice
      --max-num-seqs 4
      --max-num-batched-tokens 8192
      --served-model-name model
      --max-model-len 65536
      -tp 2
    ports:
      - "8000:8000"
    devices:
      - nvidia.com/gpu=all
