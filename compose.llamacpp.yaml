version: '3.9'
services:
  openaiapi:
    # image: ghcr.io/ggerganov/llama.cpp:server-cuda
    image: localhost/llama.cpp
    volumes:
      - /home/tiny/mnt/drive/gguf:/models
    ports:
      - "8000:8000"
    restart: unless-stopped
    # -m /models/Phi-3-medium-128k-instruct-Q8_0.gguf
    # -m /models/Phi-3.5-mini-instruct-Q8_0.gguf
    command: >
      -m /models/Phi-3-medium-128k-instruct-Q8_0.gguf
      --metrics
      --port 8000
      --host 0.0.0.0
      -ngl 100
      -fa
      -c 8192
      --log-disable
      -ts 0,1
    devices:
      - nvidia.com/gpu=all

  # chat-ui:
  #   image: localhost/chat-ui
  #   build: ./chat-ui
  #   ports:
  #     - "80:5173"
  #   restart: unless-stopped
  #   depends_on:
  #     - mongo-chatui
  #     - openaiapi

  # webprompt:
  #   image: localhost/webprompt
  #   build: ./webprompt
  #   ports:
  #     - "8004:3000"
  #   restart: unless-stopped

  # mongo-chatui:
  #   image: mongo:latest
  #   restart: unless-stopped