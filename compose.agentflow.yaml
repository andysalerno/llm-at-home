version: '3.9'
services:
  openaiapi:
    image: localhost/agentflow
    build: ./agentflow
    ports:
      - "8003:8003"
    restart: unless-stopped
    env_file:
      - ./.agentflow.default.env
      - ./.agentflow.env
    depends_on:
      - llmhost
      - embedding-server
      - scraper
    environment:
      - EMBEDDING_URI=http://embedding-server:8000/embeddings
      - SCRAPER_URI=http://scraper:8002/scrape
      - OPENAI_URI=http://llmhost:8000
      - MODEL_NAME=
    tty: true
    command: server http://llmhost:8000 http://embedding-server:8000/embeddings http://scraper:8002/scrape model -v --prompt-dir=Prompts 
    volumes:
      - ./request_logs:/App/LlmRequestLogs

  llmhost:
    image: vllm/vllm-openai:latest
    ipc: host
    environment:
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    volumes:
      - /home/tiny/mnt/drive/.cache/huggingface:/root/.cache/huggingface
      - ./tokenizer_configs/phi-3-medium-128k-instruct-template.jinja:/root/phi3_chat_template.jinja
      - ./tokenizer_configs/qwen25-instruct-v2.jinja:/root/qwenchat_template.jinja
    # restart: unless-stopped
    # --model neuralmagic/Phi-3-medium-128k-instruct-quantized.w8a8
    # --model unsloth/Llama-3.2-3B-Instruct
    # --model Qwen/Qwen2.5-14B-Instruct-AWQ
    # --chat-template /root/chat_template.jinja
    command: >
      --model Qwen/Qwen2.5-14B-Instruct-AWQ
      --served-model-name model
      --max-model-len 8192
      --chat-template /root/qwenchat_template.jinja
    ports:
      - "8000:8000"
    devices:
      - nvidia.com/gpu=all

  # llmhost:
  #   # image: ghcr.io/ggerganov/llama.cpp:server-cuda
  #   image: localhost/llama.cpp
  #   volumes:
  #     - ${MODEL_DIR}:/models
  #   restart: unless-stopped
  #   command: -m /models/${MODEL_NAME} --metrics --port 8000 --host 0.0.0.0 -ngl 100 -fa --verbose --log-disable -c 8192 -ts 1,0
  #   ports:
  #     - "8000:8000"
  #   devices:
  #     - nvidia.com/gpu=all
  #   env_file:
  #     - ./.agentflow.default.env
  #     - ./.agentflow.env

  # llmhost:
  #   image: ghcr.io/huggingface/text-generation-inference:2.3
  #   ipc: host
  #   volumes:
  #     - /home/tiny/mnt/drive/.cache/huggingface/hub:/data/hub
  #   environment:
  #     - CUDA_DEVICE_ORDER=PCI_BUS_ID
  #     - MODEL_ID=neuralmagic/Phi-3-medium-128k-instruct-quantized.w8a8
  #   command: >
  #     --model-id ${MODEL_ID}
  #     --max-top-n-tokens 1
  #     --port 8000
  #     --max-concurrent-requests 5 
  #     --max-best-of 1 
  #     --max-batch-prefill-tokens=8242 --max-total-tokens=8192 --max-input-tokens=8191
  #   ports:
  #     - "8000:8000"
  #   devices:
  #     - nvidia.com/gpu=all

  # llmhost:
  #   image: ghcr.io/theroyallab/tabbyapi:latest
  #   command: main.py --host 0.0.0.0 --port 8000 --model-name Phi-3-medium-128k-instruct-exl2-8_0 --max-seq-len 8192 --disable-auth TRUE
  #   ports:
  #     - "8000:8000"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://127.0.0.1:8000/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #   environment:
  #     - NAME=TabbyAPI
  #   volumes:
  #     - /home/tiny/mnt/drive/exl2:/app/models
  #   devices:
  #     - nvidia.com/gpu=all

  # chat-ui:
  #   image: localhost/chat-ui
  #   build: ./chat-ui
  #   ports:
  #     - "80:5173"
  #   restart: unless-stopped
  #   stop_grace_period: 1s
  #   extra_hosts:
  #     - "host.docker.internal:host-gateway"
  #   depends_on:
  #     - mongo-chatui

  embedding-server:
    image: localhost/embedding-server
    build: ./embedding-server
    ports:
      - "8001:8000"
    devices:
      - "nvidia.com/gpu=all"
    restart: unless-stopped

  scraper:
    image: localhost/scraper
    build: ./scraper
    ports:
      - "8002:8002"
    restart: unless-stopped

  # webprompt:
  #   image: localhost/webprompt
  #   build: ./webprompt
  #   ports:
  #     - "8004:3000"
  #   restart: unless-stopped

  mongo-chatui:
    image: mongo:latest
    restart: unless-stopped
    volumes:
      - mongo-storage:/data/db

  # prometheus:
  #   image: prom/prometheus:latest
  #   volumes:
  #     - ./.prometheus.config:/etc/prometheus/prometheus.yml
  #   ports:
  #     - "9090:9090"

  # grafana:
  #   image: grafana/grafana:latest
  #   ports:
  #     - "3000:3000"
  #   volumes:
  #     - grafana-storage:/var/lib/grafana
  #   depends_on:
  #     - prometheus

volumes:
  grafana-storage:
  mongo-storage:
