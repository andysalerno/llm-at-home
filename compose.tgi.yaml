services:
  llmhost:
    ipc: host
    shm_size: 1g
    image: ghcr.io/huggingface/text-generation-inference:3.3.2
    volumes:
      - /home/tiny/mnt/drive/.cache/huggingface/hub:/data/hub
    command: >
      --model-id RedHatAI/Qwen3-30B-A3B-FP8-dynamic
      --max-top-n-tokens 1
      --port 8000
      --max-concurrent-requests 1 
      --max-total-tokens 8192
      --cuda-graphs "1,2,4"
      --cuda-memory-fraction 0.9
      --max-best-of 1 
    environment:
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    ports:
      - "8000:8000"
    devices:
      - nvidia.com/gpu=all
